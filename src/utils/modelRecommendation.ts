// src/utils/modelRecommendation.ts
// -----------------------------------------------------------------------------
// Safely recommends the best local model (GPU or CPU) based on system capability.
// Includes strong fallback protection for Intel / Generic GPUs and missing f16.
// -----------------------------------------------------------------------------

// src/utils/modelRecommendation.ts
export function recommendModel(system: any) {
  const hasWebGPU = system.webgpu ?? false;
  const hasF16 = system.hasF16 ?? false;
  const gpuName = (system.gpuName || "unknown").toLowerCase();
  const deviceMemory = system.deviceMemory ?? 4;

  // Default safest CPU model
  let model = "TinyLlama-1.1B-Chat-v1.0-q4f32_1-MLC";
  let note =
    "ðŸ§© Low resource mode â€” using TinyLlama CPU-safe model for reliability.";

  // Intel / Weak GPU detection
  const isWeakGPU =
    /intel|uhd|iris|generic|hd|integrated|gfx/i.test(gpuName);

  // ðŸ§  Smart selection rules
  if (isWeakGPU || !hasWebGPU || !hasF16) {
    model = "TinyLlama-1.1B-Chat-v1.0-q4f32_1-MLC";
    note =
      "âš™ï¸ WebGPU or shader-f16 not supported â€” using TinyLlama CPU-safe model for stability.";
  } else if (hasWebGPU && hasF16 && deviceMemory >= 8) {
    model = "Phi-3-mini-4k-instruct-q4f16_1-MLC";
    note =
      "âš¡ High-performance GPU detected â€” running Phi-3 Mini optimized for GPU (f16).";
  } else if (hasWebGPU && hasF16 && deviceMemory >= 4) {
    model = "Llama-3.2-1B-Instruct-q4f16_1-MLC";
    note =
      "ðŸš€ Mid-tier GPU detected â€” using Llama 3.2 1B (GPU-optimized).";
  }

  console.log("ðŸ§  Model Recommendation:", { model, note, gpuName, hasF16 });
  return { model, note };
}
